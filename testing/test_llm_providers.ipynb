{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce4dc476",
   "metadata": {},
   "source": [
    "## How to Use in Fiesta API\n",
    "\n",
    "Once connected, use these endpoints:\n",
    "\n",
    "```bash\n",
    "# Check available models\n",
    "curl http://localhost:8000/chat/models\n",
    "\n",
    "# Send a chat message (uses tokens)\n",
    "curl -X POST http://localhost:8000/chat/ \\\n",
    "  -H \"user-id: {user_id}\" \\\n",
    "  -H \"Content-Type: application/json\" \\\n",
    "  -d '{\n",
    "    \"prompt\": \"What is AI?\",\n",
    "    \"model\": \"gemini-1.5-pro\",\n",
    "    \"max_tokens\": 500\n",
    "  }'\n",
    "\n",
    "# Check tokens remaining\n",
    "curl http://localhost:8000/subscriptions/{user_id}\n",
    "\n",
    "# Check API costs so far\n",
    "curl http://localhost:8000/admin/costs/{user_id}\n",
    "```\n",
    "\n",
    "### Token Flow in Each Request:\n",
    "\n",
    "1. **Request arrives**: User sends prompt + model choice\n",
    "2. **Check subscription**: Verify user has active subscription + tokens remaining\n",
    "3. **Check model access**: Verify user's tier allows that model\n",
    "4. **Check rate limit**: Verify user hasn't exceeded requests/minute\n",
    "5. **Estimate tokens**: Count tokens in prompt\n",
    "6. **Call API**: Send to OpenAI/Claude/Gemini\n",
    "7. **Get response**: Receive tokens_used from API\n",
    "8. **Deduct tokens**: `tokens_remaining -= tokens_used`\n",
    "9. **Track cost**: `monthly_api_cost_usd += cost`\n",
    "10. **Return response**: Send to user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3379c5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CONNECTION STATUS SUMMARY\n",
      "================================================================================\n",
      " Provider      Status Error\n",
      "   Gemini ✓ Connected  None\n",
      "Anthropic ✓ Connected  None\n",
      "   OpenAI ✓ Connected  None\n",
      "================================================================================\n",
      "\n",
      "✓ 3/3 providers are connected and working!\n",
      "\n",
      "You can now use these providers in the Fiesta API:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create summary table\n",
    "summary_data = {\n",
    "    \"Provider\": [\"Gemini\", \"Anthropic\", \"OpenAI\"],\n",
    "    \"Status\": [\n",
    "        \"✓ Connected\" if gemini_status[\"connected\"] else \"✗ Failed\",\n",
    "        \"✓ Connected\" if anthropic_status[\"connected\"] else \"✗ Failed\",\n",
    "        \"✓ Connected\" if openai_status[\"connected\"] else \"✗ Failed\"\n",
    "    ],\n",
    "    \"Error\": [\n",
    "        gemini_status[\"error\"] or \"None\",\n",
    "        anthropic_status[\"error\"] or \"None\",\n",
    "        openai_status[\"error\"] or \"None\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"CONNECTION STATUS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(df.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Count working providers\n",
    "working = sum([gemini_status[\"connected\"], anthropic_status[\"connected\"], openai_status[\"connected\"]])\n",
    "print(f\"\\n✓ {working}/3 providers are connected and working!\")\n",
    "print(\"\\nYou can now use these providers in the Fiesta API:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0284c8d8",
   "metadata": {},
   "source": [
    "## Display Connection Status Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95e28c95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ OpenAI API Connection: SUCCESS\n",
      "Response: Hello from Fiesta!\n",
      "Model: gpt-3.5-turbo-0125\n",
      "Usage: 17 input, 4 output\n"
     ]
    }
   ],
   "source": [
    "openai_status = {\"connected\": False, \"error\": None, \"response\": None}\n",
    "\n",
    "try:\n",
    "    from openai import OpenAI\n",
    "    \n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not openai_key:\n",
    "        raise Exception(\"OPENAI_API_KEY not set in .env\")\n",
    "    \n",
    "    client = OpenAI(api_key=openai_key)\n",
    "    \n",
    "    # Test request\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Say 'Hello from Fiesta!' in 3 words\"}\n",
    "        ],\n",
    "        max_tokens=100\n",
    "    )\n",
    "    \n",
    "    openai_status[\"connected\"] = True\n",
    "    openai_status[\"response\"] = response.choices[0].message.content\n",
    "    \n",
    "    print(\"✓ OpenAI API Connection: SUCCESS\")\n",
    "    print(f\"Response: {openai_status['response']}\")\n",
    "    print(f\"Model: {response.model}\")\n",
    "    print(f\"Usage: {response.usage.prompt_tokens} input, {response.usage.completion_tokens} output\")\n",
    "    \n",
    "except Exception as e:\n",
    "    openai_status[\"error\"] = str(e)\n",
    "    print(f\"✗ OpenAI API Connection: FAILED\")\n",
    "    print(f\"Error: {openai_status['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70650bd5",
   "metadata": {},
   "source": [
    "## Test OpenAI API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "07368698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Anthropic API Connection: SUCCESS\n",
      "Response: Fiesta says hello!\n",
      "Model: claude-3-haiku-20240307\n",
      "Usage: 21 input, 9 output\n"
     ]
    }
   ],
   "source": [
    "anthropic_status = {\"connected\": False, \"error\": None, \"response\": None}\n",
    "\n",
    "try:\n",
    "    from anthropic import Anthropic\n",
    "    \n",
    "    anthropic_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "    if not anthropic_key:\n",
    "        raise Exception(\"ANTHROPIC_API_KEY not set in .env\")\n",
    "    \n",
    "    client = Anthropic(api_key=anthropic_key)\n",
    "    \n",
    "    # Test request\n",
    "    message = client.messages.create(\n",
    "        model=\"claude-3-haiku-20240307\",\n",
    "        max_tokens=100,\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"Say 'Hello from Fiesta!' in 3 words\"}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    anthropic_status[\"connected\"] = True\n",
    "    anthropic_status[\"response\"] = message.content[0].text if message.content else \"No response\"\n",
    "    \n",
    "    print(\"✓ Anthropic API Connection: SUCCESS\")\n",
    "    print(f\"Response: {anthropic_status['response']}\")\n",
    "    print(f\"Model: {message.model}\")\n",
    "    print(f\"Usage: {message.usage.input_tokens} input, {message.usage.output_tokens} output\")\n",
    "    \n",
    "except Exception as e:\n",
    "    anthropic_status[\"error\"] = str(e)\n",
    "    print(f\"✗ Anthropic API Connection: FAILED\")\n",
    "    print(f\"Error: {anthropic_status['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62911bf",
   "metadata": {},
   "source": [
    "## Test Anthropic API Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3bbc3371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gemini API Connection: SUCCESS\n",
      "Model used: gemini-2.5-pro\n",
      "Response: Hello from Fiesta!\n"
     ]
    }
   ],
   "source": [
    "gemini_status = {\"connected\": False, \"error\": None, \"response\": None, \"model_used\": None}\n",
    "\n",
    "try:\n",
    "    import google.generativeai as genai\n",
    "    \n",
    "    gemini_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_key:\n",
    "        raise Exception(\"GEMINI_API_KEY not set in .env\")\n",
    "    \n",
    "    genai.configure(api_key=gemini_key)\n",
    "    \n",
    "    # Try latest model first: gemini-2.5-pro\n",
    "    model_to_try = \"gemini-2.5-pro\"\n",
    "    \n",
    "    try:\n",
    "        model = genai.GenerativeModel(model_to_try)\n",
    "        response = model.generate_content(\"Say 'Hello from Fiesta!' in 3 words\")\n",
    "        gemini_status[\"connected\"] = True\n",
    "        gemini_status[\"response\"] = response.text if hasattr(response, 'text') else str(response)\n",
    "        gemini_status[\"model_used\"] = model_to_try\n",
    "    except Exception as e1:\n",
    "        # Fallback: try gemini-2.5-flash\n",
    "        print(f\"Model {model_to_try} failed: {str(e1)}\")\n",
    "        model_to_try = \"gemini-2.5-flash\"\n",
    "        print(f\"Trying fallback model: {model_to_try}\")\n",
    "        model = genai.GenerativeModel(model_to_try)\n",
    "        response = model.generate_content(\"Say 'Hello from Fiesta!' in 3 words\")\n",
    "        gemini_status[\"connected\"] = True\n",
    "        gemini_status[\"response\"] = response.text if hasattr(response, 'text') else str(response)\n",
    "        gemini_status[\"model_used\"] = model_to_try\n",
    "    \n",
    "    print(\"✓ Gemini API Connection: SUCCESS\")\n",
    "    print(f\"Model used: {gemini_status['model_used']}\")\n",
    "    print(f\"Response: {gemini_status['response']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    gemini_status[\"error\"] = str(e)\n",
    "    print(f\"✗ Gemini API Connection: FAILED\")\n",
    "    print(f\"Error: {gemini_status['error']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3fd7ed",
   "metadata": {},
   "source": [
    "## Test Gemini API Connection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c91ec82",
   "metadata": {},
   "source": [
    "## List Available Gemini Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66a9f79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available Gemini models that support generateContent:\n",
      "============================================================\n",
      "  models/gemini-2.5-flash\n",
      "  models/gemini-2.5-pro\n",
      "  models/gemini-2.0-flash-exp\n",
      "  models/gemini-2.0-flash\n",
      "  models/gemini-2.0-flash-001\n",
      "  models/gemini-2.0-flash-lite-001\n",
      "  models/gemini-2.0-flash-lite\n",
      "  models/gemini-2.0-flash-lite-preview-02-05\n",
      "  models/gemini-2.0-flash-lite-preview\n",
      "  models/gemini-2.0-pro-exp\n",
      "  models/gemini-2.0-pro-exp-02-05\n",
      "  models/gemini-exp-1206\n",
      "  models/gemini-2.5-flash-preview-tts\n",
      "  models/gemini-2.5-pro-preview-tts\n",
      "  models/gemma-3-1b-it\n",
      "  models/gemma-3-4b-it\n",
      "  models/gemma-3-12b-it\n",
      "  models/gemma-3-27b-it\n",
      "  models/gemma-3n-e4b-it\n",
      "  models/gemma-3n-e2b-it\n",
      "  models/gemini-flash-latest\n",
      "  models/gemini-flash-lite-latest\n",
      "  models/gemini-pro-latest\n",
      "  models/gemini-2.5-flash-lite\n",
      "  models/gemini-2.5-flash-image-preview\n",
      "  models/gemini-2.5-flash-image\n",
      "  models/gemini-2.5-flash-preview-09-2025\n",
      "  models/gemini-2.5-flash-lite-preview-09-2025\n",
      "  models/gemini-3-pro-preview\n",
      "  models/gemini-3-pro-image-preview\n",
      "  models/nano-banana-pro-preview\n",
      "  models/gemini-robotics-er-1.5-preview\n",
      "  models/gemini-2.5-computer-use-preview-10-2025\n",
      "============================================================\n",
      "  models/gemini-2.5-flash\n",
      "  models/gemini-2.5-pro\n",
      "  models/gemini-2.0-flash-exp\n",
      "  models/gemini-2.0-flash\n",
      "  models/gemini-2.0-flash-001\n",
      "  models/gemini-2.0-flash-lite-001\n",
      "  models/gemini-2.0-flash-lite\n",
      "  models/gemini-2.0-flash-lite-preview-02-05\n",
      "  models/gemini-2.0-flash-lite-preview\n",
      "  models/gemini-2.0-pro-exp\n",
      "  models/gemini-2.0-pro-exp-02-05\n",
      "  models/gemini-exp-1206\n",
      "  models/gemini-2.5-flash-preview-tts\n",
      "  models/gemini-2.5-pro-preview-tts\n",
      "  models/gemma-3-1b-it\n",
      "  models/gemma-3-4b-it\n",
      "  models/gemma-3-12b-it\n",
      "  models/gemma-3-27b-it\n",
      "  models/gemma-3n-e4b-it\n",
      "  models/gemma-3n-e2b-it\n",
      "  models/gemini-flash-latest\n",
      "  models/gemini-flash-lite-latest\n",
      "  models/gemini-pro-latest\n",
      "  models/gemini-2.5-flash-lite\n",
      "  models/gemini-2.5-flash-image-preview\n",
      "  models/gemini-2.5-flash-image\n",
      "  models/gemini-2.5-flash-preview-09-2025\n",
      "  models/gemini-2.5-flash-lite-preview-09-2025\n",
      "  models/gemini-3-pro-preview\n",
      "  models/gemini-3-pro-image-preview\n",
      "  models/nano-banana-pro-preview\n",
      "  models/gemini-robotics-er-1.5-preview\n",
      "  models/gemini-2.5-computer-use-preview-10-2025\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    import google.generativeai as genai\n",
    "    \n",
    "    gemini_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "    if not gemini_key:\n",
    "        print(\"✗ GEMINI_API_KEY not set in .env\")\n",
    "    else:\n",
    "        genai.configure(api_key=gemini_key)\n",
    "        \n",
    "        print(\"Available Gemini models that support generateContent:\")\n",
    "        print(\"=\" * 60)\n",
    "        available_models = []\n",
    "        for m in genai.list_models():\n",
    "            if 'generateContent' in m.supported_generation_methods:\n",
    "                available_models.append(m.name)\n",
    "                print(f\"  {m.name}\")\n",
    "        \n",
    "        if not available_models:\n",
    "            print(\"  (No models found - check API key permissions)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error listing Gemini models: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3e7185c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Keys Status:\n",
      "==================================================\n",
      "GEMINI       ✓ SET           AIzaSyC0NF...\n",
      "ANTHROPIC    ✓ SET           sk-ant-api...\n",
      "OPENAI       ✓ SET           sk-proj-Kf...\n"
     ]
    }
   ],
   "source": [
    "api_keys = {\n",
    "    \"GEMINI\": os.getenv(\"GEMINI_API_KEY\", \"NOT SET\"),\n",
    "    \"ANTHROPIC\": os.getenv(\"ANTHROPIC_API_KEY\", \"NOT SET\"),\n",
    "    \"OPENAI\": os.getenv(\"OPENAI_API_KEY\", \"NOT SET\")\n",
    "}\n",
    "\n",
    "print(\"API Keys Status:\")\n",
    "print(\"=\" * 50)\n",
    "for provider, key in api_keys.items():\n",
    "    status = \"✓ SET\" if key != \"NOT SET\" else \"✗ NOT SET\"\n",
    "    key_preview = f\"{key[:10]}...\" if len(key) > 10 else key\n",
    "    print(f\"{provider:12} {status:15} {key_preview}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88633c26",
   "metadata": {},
   "source": [
    "## Set Up API Keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0cbed3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "print(\"✓ Libraries imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d82aca",
   "metadata": {},
   "source": [
    "## Import Required Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07faf711",
   "metadata": {},
   "source": [
    "# LLM Provider Connection Test\n",
    "\n",
    "This notebook tests connectivity to all three LLM providers (Gemini, Anthropic, OpenAI) and explains how token consumption works in the Fiesta API.\n",
    "\n",
    "## How Token Usage Works\n",
    "\n",
    "1. **User creates account** → Gets subscription (free = 1000 tokens/month)\n",
    "2. **User calls `/chat/` endpoint** → Fiesta checks if they have tokens\n",
    "3. **LLM API is called** → OpenAI/Claude/Gemini processes request\n",
    "4. **Response received** → Tokens used = prompt tokens + completion tokens\n",
    "5. **Tokens deducted** → `tokens_remaining -= tokens_used`\n",
    "6. **Cost tracked** → `monthly_api_cost_usd` increases (for billing)\n",
    "\n",
    "Each provider charges differently:\n",
    "- **OpenAI**: GPT-3.5 costs ~$0.002/request, GPT-4 ~$0.015/request\n",
    "- **Anthropic**: Claude 3 Haiku ~$0.001/request, Opus ~$0.075/request  \n",
    "- **Gemini**: Free tier, then $0.00025 per 1K input tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
